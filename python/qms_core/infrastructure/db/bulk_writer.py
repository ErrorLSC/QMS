import pandas as pd
import numpy as np
from sqlalchemy.dialects.sqlite import insert as sqlite_insert
from sqlalchemy.dialects.postgresql import insert as pg_insert
import datetime
from sqlalchemy import text,inspect
from qms_core.infrastructure.db.reader import fetch_orm_data


def smart_batch_size(df: pd.DataFrame, user_batch_size: int = None, max_params: int = 999) -> int:
    """
    æ™ºèƒ½æ‰¹é‡è®¡ç®—å‡½æ•°ï¼šè‡ªåŠ¨åœ¨æ€§èƒ½å’Œ SQLite å®‰å…¨é™åˆ¶ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡
    - è‹¥ç”¨æˆ·æŒ‡å®š batch_sizeï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨
    - å¦åˆ™è‡ªåŠ¨æ ¹æ®åˆ—æ•°å’Œæœ€å¤§SQLå‚æ•°é™åˆ¶æ¨æ–­æœ€ä¼˜æ‰¹å¤§å°
    """

    if user_batch_size:
        return user_batch_size

    num_rows = len(df)
    num_columns = max(1, len(df.columns))

    # ä¸¥æ ¼æ§åˆ¶ï¼šæ€»å˜é‡ä¸è¶…è¿‡ max_paramsï¼ˆé»˜è®¤999ï¼‰
    max_safe_batch = max_params // num_columns

    # åˆ‡å¿Œæ‰¹é‡è¿‡å°ï¼Œè®¾ç½®æ€§èƒ½ä¿åº•
    return min(num_rows, max(max_safe_batch, 25))

def write_dataframe_to_table_by_orm(
    config,
    df,
    orm_class,
    delete_before_insert: bool = False,
    upsert: bool = False,
    hot_zone_delete: bool = False,
    hot_zone_column: str = None,
    hot_zone_days: int = 180,
    batch_size: int = None,
    session=None,
    manage_transaction: bool = True
):
    if df.empty:
        print(f"âš ï¸ DataFrameä¸ºç©ºï¼Œè·³è¿‡å†™å…¥ {orm_class.__tablename__}ã€‚")
        return

    df = df.dropna(how="all")
    engine = config.get_engine()
    session = session or config.get_session()

    if delete_before_insert and upsert:
        print("âš ï¸ åŒæ—¶å¯ç”¨äº† delete_before_insert å’Œ upsertï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ‰§è¡Œ INSERT æ¨¡å¼ã€‚")
        upsert = False

    dialect = engine.dialect.name
    if dialect == "sqlite":
        insert_stmt_fn = sqlite_insert
    elif dialect == "postgresql":
        insert_stmt_fn = pg_insert
    else:
        raise NotImplementedError(f"âŒ å½“å‰æ•°æ®åº“ {dialect} ä¸æ”¯æŒè‡ªåŠ¨upsertï¼Œè¯·æ‰‹åŠ¨æ‰©å±•ï¼")

    batch_size = smart_batch_size(df, batch_size)

    def _write_core():
        if delete_before_insert:
            session.query(orm_class).delete()
            print(f"ğŸ—‘ï¸ è¡¨ {orm_class.__tablename__} å·²æ¸…ç©ºï¼Œå‡†å¤‡é‡æ–°æ’å…¥ã€‚")
        elif hot_zone_delete:
            if not hot_zone_column:
                raise ValueError("âš ï¸ å¼€å¯ hot_zone_delete æ—¶ï¼Œå¿…é¡»æŒ‡å®š hot_zone_columnã€‚")
            cutoff_date = (datetime.datetime.today() - datetime.timedelta(days=hot_zone_days)).strftime("%Y-%m-%d")
            session.execute(
                text(f"DELETE FROM {orm_class.__tablename__} WHERE {hot_zone_column} >= '{cutoff_date}'")
            )
            print(f"ğŸ—‘ï¸ è¡¨ {orm_class.__tablename__} å·²æ¸…é™¤çƒ­åŒºï¼ˆæœ€è¿‘{hot_zone_days}å¤©ï¼‰æ•°æ®ã€‚")

        valid_columns = [col.name for col in orm_class.__table__.columns]
        df_trimmed = df[[col for col in valid_columns if col in df.columns]]

        total_inserted = 0
        for i in range(0, len(df_trimmed), batch_size):
            batch_df = df_trimmed.iloc[i:i+batch_size]
            if upsert:
                insert_stmt = insert_stmt_fn(orm_class).values(batch_df.to_dict(orient="records"))
                update_columns = [
                    col.name for col in orm_class.__table__.columns
                    if not col.primary_key and col.name in batch_df.columns
                ]
                update_dict = {col: insert_stmt.excluded[col] for col in update_columns}
                upsert_stmt = insert_stmt.on_conflict_do_update(
                    index_elements=[col.name for col in orm_class.__table__.primary_key],
                    set_=update_dict
                )
                session.execute(upsert_stmt)
            else:
                records = [orm_class(**row.dropna().to_dict()) for _, row in batch_df.iterrows()]
                session.bulk_save_objects(records)

            total_inserted += len(batch_df)

        mode = "upsert" if upsert else "insert"
        print(f"âœ… æˆåŠŸ {mode} {total_inserted} æ¡è®°å½•åˆ° {orm_class.__tablename__}ï¼ˆæ¯æ‰¹æœ€å¤š{batch_size}æ¡ï¼‰")

    try:
        if manage_transaction:
            with session.begin():
                _write_core()
        else:
            _write_core()

    except Exception as e:
        session.rollback()
        print(f"âŒ å†™å…¥ {orm_class.__tablename__} å¤±è´¥: {e}")
        raise

    finally:
        if manage_transaction:
            session.close()


class SmartTableWriter:
    def __init__(self, 
                 config, orm_class, 
                 key_fields=None, monitor_fields=None, log_table_model=None, 
                 enable_logging=False,change_reason: str = "System Update",
                 exclude_fields: list[str]= None,
                 only_update_delta: bool = True,  # âœ… æ–°å¢
                 skip_if_unchanged: bool = True,  # âœ… æ–°å¢
                 write_params: dict = None):
        """
        :param config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ•°æ®åº“è¿æ¥
        :param orm_class: ORM ç±»ï¼ˆç›®æ ‡è¡¨ï¼‰
        :param key_fields: ä¸»é”®å­—æ®µååˆ—è¡¨ï¼›è‹¥ä¸ºç©ºå°†è‡ªåŠ¨ä» ORM å…ƒæ•°æ®æ¨æ–­
        :param monitor_fields: è¦ç›‘æµ‹å˜æ›´çš„å­—æ®µåˆ—è¡¨ï¼ˆç”¨äºæ¯”è¾ƒå’Œè®°å½•æ—¥å¿—ï¼‰
        :param log_table_model: ORM ç±»ï¼ˆæ—¥å¿—è¡¨ï¼‰
        :param enable_logging: æ˜¯å¦å¯ç”¨å­—æ®µå˜åŒ–æ—¥å¿—è®°å½•
        """
        self.config = config
        self.orm_class = orm_class
        self.exclude_fields = exclude_fields or [] 

        self.key_fields = key_fields or self._infer_primary_keys()
        self.monitor_fields = monitor_fields or self._infer_monitor_fields()

        self.log_table_model = log_table_model
        self.enable_logging = enable_logging
        self.change_reason = change_reason
        self.write_params = write_params or {}
        self.only_update_delta = only_update_delta
        self.skip_if_unchanged = skip_if_unchanged
        

    def _infer_primary_keys(self):
        """è‡ªåŠ¨è·å– ORM è¡¨çš„ä¸»é”®å­—æ®µå"""
        mapper = inspect(self.orm_class)
        return [col.name for col in mapper.primary_key]
    
    def _infer_monitor_fields(self) -> list[str]:
        mapper = inspect(self.orm_class)
        all_fields = [c.key for c in mapper.columns]
        pk_fields = [c.name for c in mapper.primary_key]

        monitor_fields = [f for f in all_fields if f not in pk_fields and f not in self.exclude_fields]

        print(f"â„¹ï¸ monitor_fields æœªæŒ‡å®šï¼Œé»˜è®¤ä½¿ç”¨æ‰€æœ‰éä¸»é”®å­—æ®µï¼ˆæ’é™¤å­—æ®µï¼š{self.exclude_fields}ï¼‰")
        return monitor_fields

    def write(
        self,
        df: pd.DataFrame,
        dry_run: bool = True,
        existing_df: pd.DataFrame = None,
        session=None  # âœ… å¤–éƒ¨äº‹åŠ¡æ³¨å…¥
    ) -> tuple[pd.DataFrame, pd.DataFrame]:
        """
        æ™ºèƒ½å†™å…¥ç›®æ ‡ ORM è¡¨ã€‚æ”¯æŒå¤–éƒ¨äº‹åŠ¡ï¼ˆsessionï¼‰ä¼ å…¥ã€‚
        è‹¥ dry_run=Trueï¼Œåˆ™ä¸ä¼šæ‰§è¡Œæ•°æ®åº“å†™å…¥ï¼Œä»…è¿”å›å·®å¼‚è¡Œä¸æ—¥å¿—å†…å®¹ã€‚
        """
        print("ğŸ“ å®é™…å†™å…¥å‚æ•°ï¼š", self.write_params)
        print("ğŸ“ log tableï¼š", self.log_table_model)
        if df.empty:
            print("âš ï¸ DataFrame ä¸ºç©ºï¼Œè·³è¿‡å†™å…¥ã€‚")
            return df, pd.DataFrame()

        self._external_session = session is not None
        self.session = session or self.config.get_session()

        try:
            if not self._external_session:
                self.session.begin()  # âœ… ä»…å†…éƒ¨ session å¼€å¯äº‹åŠ¡

            df_existing = self._fetch_existing(df, override_df=existing_df)

            if self.only_update_delta:
                delta_df, log_df = self._get_delta(df, df_existing)
            else:
                delta_df = df
                log_df = pd.DataFrame()

            if self.skip_if_unchanged and delta_df.empty:
                print("âœ… æ— å˜æ›´æ•°æ®ï¼Œè·³è¿‡å†™å…¥")
                return delta_df, log_df

            if dry_run:
                print(f"ğŸ§ª Dry Run æ¨¡å¼ä¸‹å‘ç° {len(delta_df)} æ¡å˜æ›´ï¼Œæœªå†™å…¥æ•°æ®åº“ã€‚")
                return delta_df, log_df

            # å†™å…¥æ—¥å¿—ï¼ˆè‹¥å¯ç”¨ï¼‰
            if self.enable_logging and self.log_table_model and not log_df.empty:
                print(f"{len(log_df)} æ¡å˜åŒ–å°†ä¼šè¢«å†™å…¥æ—¥å¿—")
                write_dataframe_to_table_by_orm(
                    config=self.config,
                    df=log_df,
                    orm_class=self.log_table_model,
                    session=self.session,
                    manage_transaction=False,
                    upsert=True
                )

            # å†™å…¥ä¸»è¡¨
            write_dataframe_to_table_by_orm(
                config=self.config,
                df=delta_df,
                orm_class=self.orm_class,
                session=self.session,
                manage_transaction=False,
                **self.write_params
            )

            if not self._external_session:
                self.session.commit()

            return delta_df, log_df

        except Exception as e:
            if not self._external_session:
                self.session.rollback()
            print("âŒ å†™å…¥å¤±è´¥ï¼Œå·²å›æ»š")
            raise

        finally:
            if not self._external_session:
                self.session.close()
                self.session = None

    def _fetch_existing(self,df:pd.DataFrame,override_df:pd.DataFrame=None) -> pd.DataFrame:
        if override_df is not None:
            return override_df
        return fetch_orm_data(self.config, self.orm_class,session = self.session)
    
    def _align_column_types(self,df1: pd.DataFrame, df2: pd.DataFrame, fields: list[str]) -> tuple[pd.DataFrame, pd.DataFrame]:
        for field in fields:
            if field in df1.columns and field in df2.columns:
                if df1[field].dtype == "object":
                    df1[field] = df1[field].apply(lambda x: x.name if hasattr(x, "value") else x)
                if df2[field].dtype == "object":
                    df2[field] = df2[field].apply(lambda x: x.name if hasattr(x, "value") else x)

                common_dtype = np.result_type(df1[field].dtype, df2[field].dtype)
                df1[field] = df1[field].astype(common_dtype)
                df2[field] = df2[field].astype(common_dtype)
        return df1, df2
    
    def _get_delta(self, df_new: pd.DataFrame, df_old: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
        if df_old.empty:
            print("BLANK!!!!")
            return df_new, pd.DataFrame()  # å…¨éƒ¨æ–°è¡Œï¼Œå†™ä¸»è¡¨ä½†æ— æ—¥å¿—

        df_new_indexed = df_new.set_index(self.key_fields)
        df_old_indexed = df_old.set_index(self.key_fields)

        # å¯æ¯”è¾ƒå­—æ®µ = é™¤å» exclude çš„æ‰€æœ‰å­—æ®µäº¤é›†
        all_fields = [f for f in df_new.columns if f not in self.key_fields and f not in self.exclude_fields]
        valid_fields = [f for f in all_fields if f in df_old.columns]

        # æ—¥å¿—å­—æ®µï¼šåœ¨ valid_fields ä¸­åˆåœ¨ monitor_fields ä¸­çš„å­—æ®µ
        monitored_fields = [f for f in self.monitor_fields if f in valid_fields]

        # å¼ºåˆ¶ç±»å‹å¯¹é½ & æšä¸¾ value åŒ–
        df_new_indexed, df_old_indexed = self._align_column_types(df_new_indexed, df_old_indexed, valid_fields)
        # print("ğŸ§ª å­—æ®µå¯¹é½å‰ç±»å‹ï¼š")
        # for f in valid_fields:
        #     dtype_new = df_new_indexed[f].dtype if f in df_new_indexed.columns else "N/A"
        #     dtype_old = df_old_indexed[f].dtype if f in df_old_indexed.columns else "N/A"
        #     print(f"  - {f}: new={dtype_new}, old={dtype_old}")

        df_old_subset = df_old_indexed[valid_fields]
        joined = df_new_indexed.join(df_old_subset, rsuffix="_old", how="left")

        matched = joined[[f"{f}_old" for f in valid_fields]].notna().all(axis=1)
        is_new = ~matched

        changed_any = pd.Series(False, index=joined.index)
        changed_monitored = pd.Series(False, index=joined.index)
        changed_fields_by_row = {}

        for field in valid_fields:
            new_val = joined[field]
            old_val = joined[f"{field}_old"]
            diff = (new_val != old_val) & ~(new_val.isna() & old_val.isna())

            changed_any |= diff
            if field in monitored_fields:
                changed_monitored |= diff

            if field in monitored_fields:
                for idx in joined.index[diff]:
                    changed_fields_by_row.setdefault(idx, []).append(field)

        # ä¸»è¡¨æ›´æ–°æ¡ä»¶ï¼šä»»ä½•å­—æ®µæœ‰å˜æ›´æˆ–æ–°è¡Œ
        mask_write_main = is_new | changed_any
        delta_df = joined[mask_write_main].reset_index()[df_new.columns]

        # æ—¥å¿—æ›´æ–°æ¡ä»¶ï¼šåªç›‘æ§å­—æ®µå˜æ›´
        log_df = pd.DataFrame()
        if self.enable_logging and self.log_table_model and changed_monitored.any():
            from datetime import datetime

            records = []
            for idx in joined.index[changed_monitored]:
                if isinstance(self.key_fields, list):
                    if isinstance(idx, tuple):
                        item_key = {k: v for k, v in zip(self.key_fields, idx)}
                    else:
                        item_key = {self.key_fields[0]: idx}
                else:
                    item_key = {self.key_fields: idx}
                for field in changed_fields_by_row.get(idx, []):
                    if field in monitored_fields:
                        records.append({
                            **item_key,
                            "FIELD_NAME": field,
                            "OLD_VALUE": joined.at[idx, f"{field}_old"],
                            "NEW_VALUE": joined.at[idx, field],
                            "CHANGE_DATE": datetime.now(),
                            "CHANGE_REASON": self.change_reason or "SmartWriter Update"
                        })

            if records:
                log_df = pd.DataFrame(records)

        return delta_df, log_df

